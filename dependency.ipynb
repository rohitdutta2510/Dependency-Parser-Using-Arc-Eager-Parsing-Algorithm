{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(file_path):\n",
    "    # Read the data from the text file\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = file.readlines()\n",
    "\n",
    "    # Initialize lists to store data\n",
    "    sent_ids = []\n",
    "    texts = []\n",
    "    tokens = []\n",
    "    POS_tags = set() # POS Tag list\n",
    "    dep_rels = set() # Dependency list\n",
    "\n",
    "    # Process the data and extract information\n",
    "    for line in data:\n",
    "        if line.startswith('# sent_id'):\n",
    "            sent_id = line.split('=')[1].strip()\n",
    "            sent_ids.append(sent_id)\n",
    "            tokens_list = []\n",
    "        elif line.startswith('# text'):\n",
    "            text = line.split('=')[1].strip()\n",
    "            texts.append(text)\n",
    "        elif line.strip() and not line.startswith('#'):\n",
    "            parts = line.strip().split()\n",
    "            POS_tags.add(parts[3]) # add the POS_tags\n",
    "            dep_rels.add(parts[5]) # add dependencies\n",
    "            tokens_list.append(' '.join(parts))\n",
    "\n",
    "        # If an empty line is encountered, add tokens to the list and reset the list\n",
    "        elif line.strip() == '' and len(tokens_list) > 0:\n",
    "            tokens.append('|'.join(tokens_list))\n",
    "\n",
    "    if len(tokens_list) > 0:\n",
    "        tokens.append('|'.join(tokens_list))\n",
    "\n",
    "\n",
    "    # Create dataframes\n",
    "    df = pd.DataFrame({'sent_id': sent_ids, 'text': texts, 'token': tokens})\n",
    "\n",
    "    return df, POS_tags, dep_rels\n",
    "\n",
    "def get_vocab(df):\n",
    "    vocab_count = {}\n",
    "    for idx, row in df.iterrows():\n",
    "        vocab = set()\n",
    "        for token in row['token'].split('|'):\n",
    "            parts = token.split(' ')\n",
    "            vocab.add(parts[2])\n",
    "\n",
    "        for word in vocab:\n",
    "            if word in vocab_count:\n",
    "                vocab_count[word] += 1\n",
    "            else:\n",
    "                vocab_count[word] = 1\n",
    "\n",
    "    # print(vocab_count['appreciation'])\n",
    "    vocab_count = {k: v for k, v in sorted(vocab_count.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "    num_sent = df.shape[0]\n",
    "    vocab = []\n",
    "    for word, count in vocab_count.items():\n",
    "        if count <= num_sent/2:\n",
    "            vocab.append(word)\n",
    "        if len(vocab) == 1000:\n",
    "            break\n",
    "    \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent_id</th>\n",
       "      <th>text</th>\n",
       "      <th>token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GUM_academic_art-1</td>\n",
       "      <td>Aesthetic Appreciation and Spanish Art:</td>\n",
       "      <td>1 Aesthetic aesthetic JJ 2 amod|2 Appreciation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GUM_academic_art-2</td>\n",
       "      <td>Insights from Eye-Tracking</td>\n",
       "      <td>1 Insights insight NNS 0 root|2 from from IN 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GUM_academic_art-3</td>\n",
       "      <td>Claire Bailey-Ross claire.bailey-ross@port.ac....</td>\n",
       "      <td>1 Claire Claire NNP 0 root|2 Bailey Bailey NNP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GUM_academic_art-4</td>\n",
       "      <td>Andrew Beresford a.m.beresford@durham.ac.uk Du...</td>\n",
       "      <td>1 Andrew Andrew NNP 0 root|2 Beresford Beresfo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GUM_academic_art-5</td>\n",
       "      <td>Daniel Smith daniel.smith2@durham.ac.uk Durham...</td>\n",
       "      <td>1 Daniel Daniel NNP 0 root|2 Smith Smith NNP 1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              sent_id                                               text  \\\n",
       "0  GUM_academic_art-1            Aesthetic Appreciation and Spanish Art:   \n",
       "1  GUM_academic_art-2                         Insights from Eye-Tracking   \n",
       "2  GUM_academic_art-3  Claire Bailey-Ross claire.bailey-ross@port.ac....   \n",
       "3  GUM_academic_art-4  Andrew Beresford a.m.beresford@durham.ac.uk Du...   \n",
       "4  GUM_academic_art-5  Daniel Smith daniel.smith2@durham.ac.uk Durham...   \n",
       "\n",
       "                                               token  \n",
       "0  1 Aesthetic aesthetic JJ 2 amod|2 Appreciation...  \n",
       "1  1 Insights insight NNS 0 root|2 from from IN 5...  \n",
       "2  1 Claire Claire NNP 0 root|2 Bailey Bailey NNP...  \n",
       "3  1 Andrew Andrew NNP 0 root|2 Beresford Beresfo...  \n",
       "4  1 Daniel Daniel NNP 0 root|2 Smith Smith NNP 1...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify the path to the text file\n",
    "train_path = 'NLP2/train.txt'\n",
    "test_path = 'NLP2/test.txt'\n",
    "\n",
    "df_train, POS_tags, dep_rels = data_loader(train_path)\n",
    "df_test, _, _ = data_loader(test_path)\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Length : 1000\n"
     ]
    }
   ],
   "source": [
    "vocab = get_vocab(df_train)\n",
    "print(f'Vocabulary Length : {len(vocab)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_map = {}\n",
    "DEP_map = {}\n",
    "VOCAB_map = {}\n",
    "\n",
    "for i, POS in enumerate(POS_tags):\n",
    "    POS_map[POS] = i\n",
    "\n",
    "for i, DEP in enumerate(dep_rels):\n",
    "    DEP_map[DEP] = i\n",
    "\n",
    "for i, VOC in enumerate(vocab):\n",
    "    VOCAB_map[VOC] = i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParserConfiguration:\n",
    "    def __init__(self, sentence):\n",
    "        self.org = sentence[:]\n",
    "        self.stack = []  # Stack initially empty\n",
    "        self.buffer = sentence[:]  # Buffer contains all tokens (shallow copy)\n",
    "        self.arcs = []  # List of tuples for arcs; each tuple is (head, dependent)\n",
    "\n",
    "    def left_arc(self, dep_rel):\n",
    "        if len(self.stack) > 0:\n",
    "            # s = self.stack.pop()  # Remove the top of the stack\n",
    "            s = self.stack[-1]  # Look at the top of the stack\n",
    "            b = self.buffer[0]  # Look at the first item in the buffer\n",
    "            \n",
    "            head_assigned = False # Check if head for first(B)\n",
    "            for arc in self.arcs: # Check if top(S) have a head word\n",
    "                if arc[1] == s[0]:\n",
    "                    head_assigned = True\n",
    "                    break\n",
    "            \n",
    "            if not head_assigned:\n",
    "                s = self.stack.pop()  # Remove the top of the stack\n",
    "                self.arcs.append((b[0], s[0], dep_rel))  # Add arc B->S with dependency relation\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "        \n",
    "        return -1\n",
    "\n",
    "    def right_arc(self, dep_rel):\n",
    "        if len(self.buffer) > 0:\n",
    "            s = self.stack[-1]  # Look at the top of the stack\n",
    "            b = self.buffer.pop(0)  # Remove the first item from the buffer\n",
    "            self.stack.append(b)  # Push B onto the stack\n",
    "            self.arcs.append((s[0], b[0], dep_rel))  # Add arc S->B with dependency relation\n",
    "\n",
    "    def reduce(self):\n",
    "        if len(self.stack) > 0:\n",
    "            s = self.stack[-1]  # Look at the top of the stack\n",
    "            head_assigned = False\n",
    "            for arc in self.arcs: # Check if top(S) have a head word\n",
    "                if arc[1] == s[0]:\n",
    "                    head_assigned = True\n",
    "                    break\n",
    "\n",
    "            if head_assigned:\n",
    "                self.stack.pop()  # Remove the top of the stack\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "            \n",
    "        return -1\n",
    "\n",
    "    def shift(self):\n",
    "        if len(self.buffer) > 0:\n",
    "            b = self.buffer.pop(0)  # Remove the first item from the buffer\n",
    "            self.stack.append(b)  # Push it onto the stack\n",
    "\n",
    "    def display_state(self):\n",
    "        print(f'\\nOriginal : {self.org}')\n",
    "        print(f'Stack : {self.stack}')\n",
    "        print(f'Buffer : {self.buffer}')\n",
    "        print(f'Arcs : {self.arcs}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureVector:\n",
    "    def __init__(self, config, VOCAB_map, POS_map, DEP_map):\n",
    "        self.V_size = len(VOCAB_map)\n",
    "        self.P_size = len(POS_map)\n",
    "        self.R_size = len(DEP_map)\n",
    "        self.feature_vector_size = 4 * (2*self.V_size + 3*self.P_size + 4*self.R_size)\n",
    "        self.f_vec = np.zeros(self.feature_vector_size, dtype=int)\n",
    "        self.S, self.B, self.A = config.stack, config.buffer, config.arcs\n",
    "        self.conf = config\n",
    "        self.VOCAB_map = VOCAB_map\n",
    "        self.POS_map = POS_map\n",
    "        self.DEP_map = DEP_map\n",
    "\n",
    "    # Helper function to update feature vector for a given feature index\n",
    "    def set_feature(self, idx):\n",
    "        if 0 <= idx < self.feature_vector_size:\n",
    "            self.f_vec[idx] = 1\n",
    "    \n",
    "    def update_top_S_DEP(self, top_S, offset):\n",
    "        head_top_S = top_S[3]\n",
    "        left_most_w = self.conf.org[0]\n",
    "        right_most_w = self.conf.org[-1]\n",
    "\n",
    "        for (head, dep, rel) in self.A:\n",
    "            if rel != None:\n",
    "                if head == head_top_S and dep == top_S[0]:\n",
    "                    self.set_feature(self.V_size + self.P_size + self.DEP_map[rel] + offset)\n",
    "                elif head == top_S[0] and dep == left_most_w[0]:\n",
    "                    self.set_feature(self.V_size + self.P_size + self.R_size + self.DEP_map[rel] + offset)\n",
    "                elif head == top_S[0] and dep == right_most_w[0]:\n",
    "                    self.set_feature(self.V_size + self.P_size + 2*self.R_size + self.DEP_map[rel] + offset)\n",
    "\n",
    "    def update_buffer_DEP(self,first_B, offset):\n",
    "        left_most_w = self.conf.org[0]\n",
    "        for (head, dep, rel) in self.A:\n",
    "            if rel != None:\n",
    "                if head == first_B[0] and dep == left_most_w[0]:\n",
    "                    self.set_feature(2*self.V_size + 2*self.P_size + 3*self.R_size + self.DEP_map[rel] + offset)\n",
    "\n",
    "\n",
    "    def get_feature_vector(self, transition):   \n",
    "        # Feature indices based on the transition\n",
    "        transition_map = {'LEFT-ARC': 0, 'RIGHT-ARC': 1, 'REDUCE': 2, 'SHIFT': 3}\n",
    "        transition_offset = transition_map[transition] * (2*self.V_size + 3*self.P_size + 4*self.R_size)\n",
    "        \n",
    "        # TOP feature\n",
    "        if self.S:\n",
    "            top_S = self.S[-1]  # Last stack item is the top\n",
    "            # print(top_S)\n",
    "\n",
    "            # TOP\n",
    "            top_token = top_S[1]  # Normalized token\n",
    "            if top_token in self.VOCAB_map:\n",
    "                self.set_feature(self.VOCAB_map[top_token] + transition_offset)\n",
    "            \n",
    "            # TOP.POS\n",
    "            top_POS = top_S[2] # POS_tag for TOS\n",
    "            if top_POS in self.POS_map:\n",
    "                self.set_feature(self.V_size + self.POS_map[top_POS] + transition_offset)\n",
    "            \n",
    "            # Update DEP features : TOP.DEP, TOP.RDEP, TOP.LDEP\n",
    "            self.update_top_S_DEP(top_S, transition_offset)\n",
    "        \n",
    "        # FIRST feature\n",
    "        if self.B:\n",
    "            first_B = self.B[0]  # First buffer item \n",
    "            \n",
    "            # FIRST\n",
    "            first_token = first_B[1] # Normalized token\n",
    "            if first_token in self.VOCAB_map:\n",
    "                self.set_feature(self.V_size + self.P_size + 3 * self.R_size + self.VOCAB_map[first_token] + transition_offset)\n",
    "            \n",
    "            # FIRST.POS\n",
    "            first_POS = first_B[2] # POS_tag for first(Buffer)\n",
    "            if first_POS in self.POS_map:\n",
    "                self.set_feature(2 * self.V_size + self.P_size + 3 * self.R_size + self.POS_map[first_POS] + transition_offset)\n",
    "            \n",
    "            # Update DEP features : FIRST.LDEP\n",
    "            self.update_buffer_DEP(first_B, transition_offset)\n",
    "        \n",
    "        # LOOK.POS\n",
    "        if len(self.B) >= 2:\n",
    "            second_B = self.B[1]\n",
    "            second_POS = second_B[2]\n",
    "            self.set_feature(2 * self.V_size + 2 * self.P_size + 4 * self.R_size + self.POS_map[second_POS] + transition_offset)\n",
    "        \n",
    "        return self.f_vec\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_REDUCE(stack, buffer, top_of_stack , gold_arcs, existing_arcs):\n",
    "    '''\n",
    "    stack = list[]\n",
    "    buffer = list[]\n",
    "    top_of_stack : int (index of TOS)\n",
    "    gold_arcs : dict{}\n",
    "    existing_arcs : dict{}\n",
    "    '''\n",
    "    exists_dep = False\n",
    "    for arcs in existing_arcs:\n",
    "        if top_of_stack == arcs[1]: # head of top_of_stack has been assigned\n",
    "            for element in stack: # check for a 'w' in the stack\n",
    "                ele_id = element[0]\n",
    "                if ele_id != top_of_stack: # w != top(S)\n",
    "                    if (ele_id, buffer[0][0]) in gold_arcs or (buffer[0][0], ele_id) in gold_arcs:\n",
    "                        exists_dep = True\n",
    "                        break\n",
    "    \n",
    "    return exists_dep\n",
    "\n",
    "def oracle(conf, gold_arcs):\n",
    "    S, B, A = conf.stack, conf.buffer, conf.arcs\n",
    "    \n",
    "    if not S:\n",
    "        return 'SHIFT', None  # If stack is empty, we can only SHIFT.\n",
    "    \n",
    "    top_of_stack = S[-1][0]  # Assuming each item in S and B is [token_id, ...]\n",
    "    first_of_buffer = B[0][0] if B else None\n",
    "\n",
    "    # Convert set of arcs A to a more searchable structure\n",
    "    existing_arcs = {(head, dep) for head, dep, rel in A}\n",
    "\n",
    "    # Rule 1: LEFT-ARC\n",
    "    if first_of_buffer and (first_of_buffer, top_of_stack) in gold_arcs: # If first(B) -> top(S) exists in the dependency graph\n",
    "        head_assigned = False\n",
    "        for arcs in existing_arcs: # Check if top(S) already has a head\n",
    "            if top_of_stack == arcs[1]:\n",
    "                head_assigned = True\n",
    "                break\n",
    "        if not head_assigned:\n",
    "            return 'LEFT-ARC', gold_arcs[(first_of_buffer, top_of_stack)]\n",
    "    \n",
    "    # Rule 2: RIGHT-ARC\n",
    "    if first_of_buffer and (top_of_stack, first_of_buffer) in gold_arcs:\n",
    "        return 'RIGHT-ARC', gold_arcs[(top_of_stack, first_of_buffer)]\n",
    "    \n",
    "    # Rule 3: REDUCE\n",
    "    if check_REDUCE(S, B, top_of_stack, gold_arcs, existing_arcs):\n",
    "        return 'REDUCE', None\n",
    "    \n",
    "    # Default to SHIFT if none of the other conditions are met.\n",
    "    return 'SHIFT', None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_instances(sentences, oracle):\n",
    "    training_instances = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        # Initial configuration for the sentence\n",
    "        conf = ParserConfiguration(sentence)\n",
    "        \n",
    "        # Extract gold-standard arcs from the sentence\n",
    "        gold_arcs = {(word[3], word[0]): word[4] for word in sentence}\n",
    "\n",
    "        while len(conf.buffer) >= 1:  # Continue until buffer is empty \n",
    "            gold_action, dep_rel = oracle(conf, gold_arcs)\n",
    "            # Encode current configuration and gold action into feature vector\n",
    "            training_instances.append((conf, gold_action))\n",
    "            \n",
    "            # Update configuration based on the gold action\n",
    "            if gold_action == 'LEFT-ARC':\n",
    "                conf.left_arc(dep_rel)\n",
    "            elif gold_action == 'RIGHT-ARC':\n",
    "                conf.right_arc(dep_rel)\n",
    "            elif gold_action == 'REDUCE':\n",
    "                conf.reduce()\n",
    "            elif gold_action == 'SHIFT':\n",
    "                conf.shift()\n",
    "        \n",
    "    return training_instances\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(training_data, num_features, num_epochs, VOCAB_map, POS_map, DEP_map):\n",
    "    \"\"\"\n",
    "    Trains a linear classifier using the Perceptron algorithm.\n",
    "    \n",
    "    training_data: A list of tuples (feature_vector, gold_action) for training.\n",
    "    num_features: The number of features in each feature vector.\n",
    "    num_epochs: The number of passes over the training data.\n",
    "    A NumPy array representing the trained weights.\n",
    "    \"\"\"\n",
    "    actions = ['LEFT-ARC', 'RIGHT-ARC', 'REDUCE', 'SHIFT']\n",
    "\n",
    "    index_to_action = {i: action for i, action in enumerate(actions)}\n",
    "    W = np.random.rand(num_features)\n",
    "\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for conf, gold_action in training_data:\n",
    "            # Predict action\n",
    "            scores = []\n",
    "            for test_action in actions:\n",
    "                fv = FeatureVector(conf, VOCAB_map, POS_map, DEP_map)\n",
    "                f_vec = fv.get_feature_vector(test_action)\n",
    "                score = np.dot(W, f_vec)\n",
    "                scores.append(score)\n",
    "\n",
    "            predicted_action_index = np.argmax(scores)\n",
    "            predicted_action = index_to_action[predicted_action_index]\n",
    "            \n",
    "            # Update weights if prediction is wrong\n",
    "            if predicted_action != gold_action:\n",
    "                feature_vector_pred = fv.get_feature_vector(predicted_action)\n",
    "                feature_vector_gold = fv.get_feature_vector(gold_action)\n",
    "                W = W + feature_vector_gold - feature_vector_pred\n",
    "\n",
    "        print(f'Epoch : {epoch + 1}')\n",
    "                \n",
    "    return W\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_action(sentences, weights, POS_DEP_map):\n",
    "    actions = ['LEFT-ARC', 'RIGHT-ARC', 'REDUCE', 'SHIFT']\n",
    "    index_to_action = {i: action for i, action in enumerate(actions)}\n",
    "    arc_list = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # print('Parsing', count)\n",
    "        conf = ParserConfiguration(sentence)\n",
    "        while len(conf.buffer) >= 1:\n",
    "            scores = []\n",
    "            for test_action in actions:\n",
    "                fv = FeatureVector(conf, VOCAB_map, POS_map, DEP_map)\n",
    "                f_vec = fv.get_feature_vector(test_action)\n",
    "                score = np.dot(weights, f_vec)\n",
    "                scores.append(score)\n",
    "            \n",
    "            predicted_action_index = np.argmax(scores)\n",
    "            predicted_action = index_to_action[predicted_action_index]\n",
    "            \n",
    "            if predicted_action == 'LEFT-ARC':\n",
    "                top_S_POS = None\n",
    "                first_B_POS = None\n",
    "                dep = None\n",
    "\n",
    "                if len(conf.stack) > 0:\n",
    "                    top_S_POS = conf.stack[-1][2]\n",
    "                first_B_POS = conf.buffer[0][2]\n",
    "\n",
    "                if (first_B_POS, top_S_POS) in POS_DEP_map:\n",
    "                    dep = POS_DEP_map[(first_B_POS, top_S_POS)]\n",
    "\n",
    "                flag = conf.left_arc(dep)\n",
    "                if flag == -1 : # Empty stack heuristic\n",
    "                    conf.shift()\n",
    "                if flag == 0: # Head already assigned Heuristic\n",
    "                    conf.reduce()\n",
    "\n",
    "            elif predicted_action == 'RIGHT-ARC':\n",
    "                if len(conf.stack) > 0:\n",
    "                    top_S_POS = conf.stack[-1][2]\n",
    "                    first_B_POS = conf.buffer[0][2]\n",
    "                    \n",
    "                    if (top_S_POS, first_B_POS) in POS_DEP_map:\n",
    "                        dep = POS_DEP_map[(top_S_POS, first_B_POS)]\n",
    "\n",
    "                    conf.right_arc(dep)\n",
    "                else:\n",
    "                    conf.shift() # Heuristic\n",
    "\n",
    "            elif predicted_action == 'REDUCE':\n",
    "                flag = conf.reduce()\n",
    "\n",
    "                if flag == -1: # Empty stack heuristic\n",
    "                    conf.shift()\n",
    "                if flag == 0: # Head not assigned Heuristic\n",
    "                    top_S_POS = conf.stack[-1][2]\n",
    "                    first_B_POS = conf.buffer[0][2]\n",
    "\n",
    "                    if (first_B_POS, top_S_POS) in POS_DEP_map:\n",
    "                        dep = POS_DEP_map[(first_B_POS, top_S_POS)]\n",
    "\n",
    "                    conf.left_arc(dep)\n",
    "\n",
    "            elif predicted_action == 'SHIFT':\n",
    "                conf.shift()\n",
    "        \n",
    "\n",
    "        arc_list.append([(arc[0], arc[1]) for arc in conf.arcs])\n",
    "        # arc_list.append(conf.arcs)\n",
    "\n",
    "    return arc_list\n",
    "\n",
    "def calculate_UAS(arc_list, gold_arcs):\n",
    "    scores = []\n",
    "    for idx, sent in enumerate(gold_arcs):\n",
    "        correct = 0\n",
    "        for arc in sent:\n",
    "            if arc in arc_list[idx]:\n",
    "                correct += 1\n",
    "        scores.append(correct/len(gold_arcs[idx]))\n",
    "\n",
    "    return sum(scores)/len(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_POS_approximation(sentences):\n",
    "    POS_pair_DEP = {}\n",
    "    for sentence in sentences:\n",
    "        id_to_POS = {}\n",
    "        for token in sentence:\n",
    "            id_to_POS[token[0]] = token[2]\n",
    "\n",
    "        for token in sentence:\n",
    "            dep = token[0]\n",
    "            head = token[3]\n",
    "            rel = token[4]\n",
    "            if head != '0' and head != '_':\n",
    "                if (id_to_POS[head], id_to_POS[dep]) in POS_pair_DEP:\n",
    "                    DEP_list = POS_pair_DEP[(id_to_POS[head], id_to_POS[dep])]\n",
    "                    \n",
    "                    if rel in DEP_list:\n",
    "                        POS_pair_DEP[(id_to_POS[head], id_to_POS[dep])][rel] += 1\n",
    "                    else:\n",
    "                        POS_pair_DEP[(id_to_POS[head], id_to_POS[dep])][rel] = 1\n",
    "\n",
    "                else:\n",
    "                    POS_pair_DEP[(id_to_POS[head], id_to_POS[dep])] = {}\n",
    "                    POS_pair_DEP[(id_to_POS[head], id_to_POS[dep])][rel] = 1\n",
    "    \n",
    "    POS_DEP_map = {}\n",
    "    for POS_pair, DEP_dict in POS_pair_DEP.items():\n",
    "        dep = max(DEP_dict, key=DEP_dict.get)\n",
    "        POS_DEP_map[POS_pair] = dep\n",
    "\n",
    "    return POS_DEP_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_true_arcs(sentences):\n",
    "    true_arcs_list = []\n",
    "    for sentence in sentences:\n",
    "        true_arcs_list.append([(word[3], word[0]) for word in sentence])\n",
    "\n",
    "    return true_arcs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the sentences for the train and test\n",
    "\n",
    "train_sentences = []\n",
    "for idx, row in df_train.iterrows():\n",
    "    sentence = []\n",
    "    tok_list = row['token'].split('|')\n",
    "    for tok in tok_list:\n",
    "        parts = tok.split()\n",
    "        final_token = [parts[0], parts[2], parts[3], parts[4], parts[5]]\n",
    "        sentence.append(final_token)\n",
    "    train_sentences.append(sentence)\n",
    "\n",
    "test_sentences = []\n",
    "for idx, row in df_test.iterrows():\n",
    "    sentence = []\n",
    "    tok_list = row['token'].split('|')\n",
    "    for tok in tok_list:\n",
    "        parts = tok.split()\n",
    "        final_token = [parts[0], parts[2], parts[3], parts[4], parts[5]]\n",
    "        sentence.append(final_token)\n",
    "    test_sentences.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_instances  = generate_training_instances(train_sentences, oracle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1\n",
      "Epoch : 2\n",
      "Epoch : 3\n",
      "Epoch : 4\n",
      "Epoch : 5\n",
      "Epoch : 6\n",
      "Epoch : 7\n",
      "Epoch : 8\n",
      "Epoch : 9\n",
      "Epoch : 10\n",
      "Epoch : 11\n",
      "Epoch : 12\n",
      "Epoch : 13\n",
      "Epoch : 14\n",
      "Epoch : 15\n",
      "Epoch : 16\n",
      "Epoch : 17\n",
      "Epoch : 18\n",
      "Epoch : 19\n",
      "Epoch : 20\n",
      "Epoch : 21\n",
      "Epoch : 22\n",
      "Epoch : 23\n",
      "Epoch : 24\n",
      "Epoch : 25\n",
      "Epoch : 26\n",
      "Epoch : 27\n",
      "Epoch : 28\n",
      "Epoch : 29\n",
      "Epoch : 30\n",
      "Epoch : 31\n",
      "Epoch : 32\n",
      "Epoch : 33\n",
      "Epoch : 34\n",
      "Epoch : 35\n",
      "Epoch : 36\n",
      "Epoch : 37\n",
      "Epoch : 38\n",
      "Epoch : 39\n",
      "Epoch : 40\n",
      "Epoch : 41\n",
      "Epoch : 42\n",
      "Epoch : 43\n",
      "Epoch : 44\n",
      "Epoch : 45\n",
      "Epoch : 46\n",
      "Epoch : 47\n",
      "Epoch : 48\n",
      "Epoch : 49\n",
      "Epoch : 50\n"
     ]
    }
   ],
   "source": [
    "num_features = 4 * (2 * len(VOCAB_map) + 3 * len(POS_map) + 4 * len(DEP_map))\n",
    "weights = train_classifier(training_instances, num_features, 50, VOCAB_map, POS_map, DEP_map)\n",
    "np.save('dependency_predictions_on.npy', weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UAS Train Set: 0.14903587485786365\n",
      "UAS Test Set: 0.1478183209857962\n"
     ]
    }
   ],
   "source": [
    "POS_DEP_map = get_POS_approximation(train_sentences)\n",
    "\n",
    "train_pred_arcs = predict_action(train_sentences, weights, POS_DEP_map)\n",
    "train_true_arcs = get_true_arcs(train_sentences)\n",
    "UAS_train = calculate_UAS(train_pred_arcs, train_true_arcs)\n",
    "print(f'UAS Train Set: {UAS_train}')\n",
    "\n",
    "test_pred_arcs = predict_action(test_sentences, weights, POS_DEP_map)\n",
    "test_true_arcs = get_true_arcs(test_sentences)\n",
    "UAS_test = calculate_UAS(test_pred_arcs, test_true_arcs)\n",
    "print(f'UAS Test Set: {UAS_test}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('dependency_predictions_on.tsv', mode='w+', newline='', encoding=\"utf-8\") as file:    \n",
    "    writer = csv.writer(file, delimiter='\\t', lineterminator='\\n')\n",
    "    for idx, arc_list in enumerate(test_pred_arcs):\n",
    "        for arc in arc_list:\n",
    "            head, dep = arc\n",
    "            \n",
    "            for tok in test_sentences[idx]:\n",
    "                if tok[0] == dep:\n",
    "                    writer.writerow([df_test.iloc[idx][\"sent_id\"], dep, tok[1], head])\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
